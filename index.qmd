---
title: "Statistische Modellierung mit R"
subtitle: "Ein Workshop für die PH Zürich"
date: 2025-10-17
format:
  html:
    toc: true
    toc-depth: 4
    self-contained: true
    grid: 
      margin-width: 350px
brand:
  color:
    link: "#1bbc9d"
execute: 
  echo: fenced
lang: de
reference-location: margin
#citation-location: margin
bibliography: skeleton.bib
---

# Methodology: The Science before Statistics

Jede statistische Modellierung gewinnt an Aussagekraft, je umfassender sie die inhaltliche Fragestellung abzubilden im Stande ist. Um aus der riesigen Fülle an Optionen geeignet und zielgerichtet auswählen zu können sind die folgenden Unterscheidungen hilfreich.

## Erkenntnisinteressen

Ganz grundlegend kann a priori das Erkenntnisinteresse von Studien in die folgenden vier Kategorien unterschieden werden:

::: column-page-right
| Deskriptiv | Explorativ | Explanativ | Prädiktiv |
|------------------|------------------|------------------|------------------|
| populationsbeschreibend | hypothesengenerierend | hypothesenprüfend | Datenpunkte vorhersagend oder imputierend |
| *Bei welchem Anteil 15-Jähriger in Deutschland handelt es sich um funktionale Analphabet:innen?* | *Was sind potentielle Ursachen für genderbezogene Disparitäten im Analphabetismus?* | *Sind 15-jährige Jungen häufiger Analphabeten als 15-jährige Mädchen?* | *Mit welchen Variablen können Schüler:innen at risk erfolgreich identifiziert werden?* |

: Erkenntisinteressen nach [@doering2016].
:::

## Gütekriterien wiss. Erkenntnis nach @campbell1957

Für ein erfolgreiches Studiendesign und die anschließende statistische Analyse ist es sehr wertvoll sich vorab über Schwerpunkte besonders gewünschter Aspekte wissenschaftlicher Güte Gedanken zu machen. Insbesondere über die Unterkriterien **Methodischer Strenge**

-   **Konstruktvalidität** (*Inwiefern ist die Interpretation der Messwerte angemessen?*)
-   **Interne Validität** (*Inwiefern sind Assoziationen von unabhängiger \[beeinflussender\] und abhängiger \[beeinflusster\] Variabler als kausale Effekte interpretierbar?*)
-   **Externe Validität** (*Inwiefern können die Schlussfolgerungen der Studie verallgemeinert werden?*)
-   **Statistische Validität** (*Wie robust und angemessen sind die verwendeten statistischen Verfahren?*)

## Deskriptiv- und Inferenzstatistik

**Deskriptive Statistik** beschreibt vorliegende Daten (z.B. Effektstärken), während **Inferenzstatistik** Aussagen über den die Daten generierenden Mechanismus trifft. Beide können »eher einfach« oder »hoch komplex« sein und oftmals sethen sie in einem synergetischen Verhältnis (siehe @fig-jingjang).

::: column-margin
![Verhältnis von deskriptiver und Inferenzstatistik"](img/yingyang.svg){#fig-jingjang width=300px}
:::

## Bayesianisches und Frequentistisches Schätzen und Testen
Eine sehr heuristische Klassifikation inferenzstatistischer Verfahren stellt die Unterscheidung von statistischer Schätzung und Testung:

> (Inferenzstatistische) *Schätzungen* (estimation with quantified uncertainty) treffen anhand von Stichproben Aussagen über Parameter der Grundgesamtheit (Population) aus der die Stichprobe gezogen wurde.[^1]

[^1]: Bspw.: *Mit 96%er Wahrscheinlichkeit liegt die Analphabetismusinzidienz von 15-Jährigen in Deutschland zwischen .08 und .12* 

> (Inferenzstatistische) *Hypothesentests* bewerten anhand von Stichprobendaten die Gültigkeit von Hypothesen in der Grundgesamtheit (Population) aus der die Stichprobe gezogen wurde.[^2]

[^2]: Bspw.: *Nimmt man an, dass sich die Analphabetismusinzidienz von 15-Jährigen in Deutschland zwischen 2021 und 2025 nicht geändert hat beträgt die Wahrscheinlichkeit der vorliegenden Daten p = .032* 

Diese beiden Verfahren können sowohl im Rahmen der **frequentistischen Statistik** als auch der **bayesianischen Statistik** angewendet werden. Die folgende Tabelle gibt einen Überblick über die wichtigsten Werkzeuge:

|                        | Frequentistische Statistik | Bayesianische Statistik |
|------------------------|-----------------------------|--------------------------|
| Parameterschätzung     | Konfidenzintervalle         | Posterior Distributions  |
| Hypothesentest         | p-Werte                     | Bayes Faktoren & ROPE Procedure           |

## Hypothesenarten
Bayesianische wie frequentistischen Hypothesentests können unterschiedliche Arten von Hypothesen zugrunde gelegt werden:

* **Punkthypothesen** setzen Parameter gleich einer reellen Zahl; etwa $H_0\text{: } \delta = 0$ 
* **Äquivalenzhypothesen** nehmen Parameter in einem reellen Intervall an; etwa  $H_0\text{: } \delta \not\in\ [-.3, .3]$ 
* **Informative Hypothesen** nehmen eine Ordnungsrelation mehrerer Parameter an; etwa $\mu_{\text{Baseline}} < \mu_{\text{Imaginary Pill}} < \mu_{\text{Blinded Placebo}}$ [@buergler2023]

> **Die *Art* der (falsifizierten) Hypothese entscheidet wesentlich stärker über den Informationsgehalt eines Hypothesentests als die Entscheidung für das frequentistische oder bayesianische Paradigma** [@hoijtink2012].

Dies ist am leichtest anhand der Nullhypothese nachvollziehbar. Wird etwa die Nullhypothese $H_0\text{: } \delta = 0$ verworfen, wird entsprechend die Alternativhypothese $H_A\text{: } \delta \neq 0$ angenommen. Diese enthält aber quasi keine Information, da sie nur mit einer einzigen Beobachtung (d = 0.000000 ...) verworfen werden kann und im kritischen Rationalismus gilt, dass eine Aussage umso mehr Information enthält, umso leichter sie verworfen werden kann [@doering2016]. 

Äquivalenzhypothesen können sowohl frequentistisch [z.B. TOAST-Prozedur in R und JASP, @lakens2017] wie bayesianisch [z.B. ROPE-Ansatz @kruschke2015] getestet werden. Für das Testen informativer Hypothesen liegen bayesianische Methoden in (u.a.) JASP und R vor [z.B. `{bain}`, @gu2019] sowie in frequentistischen R-Paketen restriktor [@vanbrabant2020] und ic.infer [@gromping2010].

# Grundlagen der Regressionsanalyse
Regressionsanalysen sind ein sehr mächtiges Werkzeug um Zusammenhänge oder Unterschiede zwischen/in Variablen zu modellieren. Innerhalb der Regressionsanalyse kann sowohl geschätz als auch getestet werden und dies jeweils bayesianisch oder frequentistisch.

Die Grundidee der lineare Regression ist in [diesem interaktiven Applet](https://www.geogebra.org/m/wDpDdS7g) veranschaulicht.
Eine Abhängige Variable $y_i$ wird also als normalverteilt mit bedingtem Erwartungswert $b_0 + b_1*x_i$ und  $\sigma$ angenommen:

$$y_i \sim \mathcal{N}\left( b_0 + b_1*x_i, \sigma \right)$$


## Datengrundlage
Das wollen wir den berühmten Daten aus dem STAR-Experiment (Student/Teacher Achievement Ratio) illustrieren. In diesem Experiment wurden in Tennessee Schüler:innen zufällig auf Klassen mit kleiner (13-17 Schüler:innen pro Lehrer:in) und großer (22-25 Schüler:innen pro Lehrer:in) Klassengröße zugeteilt. Zusätzlich gab es Klassen großer Größe, die von einer:n ausgebildeten Hilfslehrer:in unterstützt wurde. Die Schüler:innen wurden über mehrere Jahre getestet.

Wir können die Daten [herunterladen](https://github.com/sammerk/Zuerich_Statistics/tree/master/data) oder direkt programmatisch importieren.

```{r}
library(readr) # für den data import
library(dplyr)    # für das datawrangling
data_star <-
  readr::read_csv(
    paste0(
      "https://raw.githubusercontent.com/sammerk/",
      "Zuerich_Statistics/master/data/star.csv"
    )
  )
```

Der Satzsatz sieht wie folgt aus:

```{r}
glimpse(data_star)
```

Die Variablen dieses Datensatzes sind die folgenden:

* `id`: a factor - student id number
* `sch`: a factor - school id number
* `gr`: grade - an ordered factor with levels K < 1 < 2 < 3
* `cltype`: class type - a factor with levels small, reg and reg+A. The last level indicates a regular class size with a teachers aide.
* `hdeg`: highest degree obtained by the teacher - an ordered factor with levels ASSOC < BS/BA < MS/MA/MEd < MA+ < Ed.S < Ed.D/Ph.D
* `clad`: career ladder position of the teacher - a factor with levels NOT APPR PROB PEND 1 2 3
* `exp`: a numeric vector - the total number of years of experience of the teacher
* `trace`: teacher's race - a factor with levels W, B, A, H, I and O representing white, black, Asian, Hispanic, Indian (Native American) and other
* `read`: the student's total reading scaled score
* `math`: the student's total math scaled score
* `ses`: socioeconomic status - a factor with levels F and N representing eligible for free lunches or not eligible
* `schtype`: school type - a factor with levels inner, suburb, rural and urban
* `sx`: student's sex - a factor with levels M F
* `eth`: student's ethnicity - a factor with the same levels as trace
* `birthq`: student's birth quarter - an ordered factor with levels 1977:1 < ... < 1982:2
* `birthy`: student's birth year - an ordered factor with levels 1977:1982
* `yrs`: number of years of schooling for the student - a numeric version of the grade gr with Kindergarten represented as 0. This variable was generated from gr and does not allow for a student being retained.
* `tch`: a factor - teacher id number


Um nicht gleich mit Mehrebenenregression starten zu müssen, erstellen wir als erstes einen Subdatensatz, der nur eine:n Schüler:in pro Lehrer:in enthält

```{r}
data_star_sub <- 
	data_star %>% 
	group_by(tch) %>% 
	sample_n(1) %>% 
	ungroup()
```

## »Effekte« der Klassenstufe
### Einfache lineare Regression
Zunächst wollen wir »Effekte« der Klassenstufe modellieren. Ich empfehle dringend kosequent Daten erst zu visualisieren und dann statistisch zu modellieren.

```{r}
library(ggplot2)  # plots
library(ggforce)  # sina plots

ggplot(data_star_sub, aes(gr, math)) +
	geom_jitter() +
	theme_minimal()
```

Die unabhängige Variable ist hier noch nicht intervallskaliert. Dies können wir wie folgt ändern.

```{r}
# Das Schuljahr K zu 0 rekodieren
data_star_sub <- 
	data_star_sub %>% 
	mutate(grade = case_when(gr == "K" ~ 0,
													 gr == "1" ~ 1,
													 gr == "2" ~ 2,
													 gr == "3" ~ 3
													 ))
# Anschließend nochmals visualisieren
ggplot(data_star_sub, aes(grade, math)) +
	geom_jitter() +
	theme_minimal()
```


Dann kann eine erste Regressionsgerade modelliert werden.

#### Frequentistische Punktschätzung
Mit der Funktion `lm()` werden mithilfe kleinster Quadrate $b_0$ und $b_1$ geschätzt. 

```{r}
mod0 <- lm(math ~ grade, data = data_star_sub)
mod0
```

#### Frequentistischer Nullhypothesensignifikanztest
Die Standardinferenzstatistik zu einem solchen Modell ist sind p-Werte für die Nullhypothesen 
$$H_0:\; b_0 = 0$$
$$H_0:\; b_1 = 0$$

```{r}
summary(mod0)
```

#### Frequentistische Konfidenzintervalle
```{r}
confint(mod0)
```

#### Bayesianische Posterior Distributions
```{r}
library(MCMCpack) #für bayesianische Schätzung
mod1 <- MCMCregress(math ~ grade, data = data_star_sub)

plot(mod1)
summary(mod1)
```

Während die Densityplots den Posterior der Schätzung Charakterisieren erhält man mit `summary()` auch die 95%HDI-Intervalle.

